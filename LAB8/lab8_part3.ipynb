{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of lab8 part3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbxEDU_NPCFl",
        "colab_type": "text"
      },
      "source": [
        "1. Construct the CNN network, then train the CNN with the full dataset, and test the trained CNN, the accuracy is 0.9903.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJRHPND9E1th",
        "colab_type": "code",
        "outputId": "dd035219-c581-4b31-bad9-9a5da0d2a644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains a simple convnet on the MNIST dataset.\n",
        "Gets to 99.25% test accuracy after 12 epochs\n",
        "(there is still a lot of margin for parameter tuning).\n",
        "16 seconds per epoch on a GRID K520 GPU.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.2757 - acc: 0.9150 - val_loss: 0.0597 - val_acc: 0.9806\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0900 - acc: 0.9733 - val_loss: 0.0435 - val_acc: 0.9869\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0660 - acc: 0.9800 - val_loss: 0.0363 - val_acc: 0.9876\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0558 - acc: 0.9831 - val_loss: 0.0329 - val_acc: 0.9895\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0477 - acc: 0.9856 - val_loss: 0.0302 - val_acc: 0.9899\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0421 - acc: 0.9868 - val_loss: 0.0328 - val_acc: 0.9894\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0390 - acc: 0.9878 - val_loss: 0.0282 - val_acc: 0.9909\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0352 - acc: 0.9889 - val_loss: 0.0256 - val_acc: 0.9916\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 145us/step - loss: 0.0325 - acc: 0.9898 - val_loss: 0.0293 - val_acc: 0.9899\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0294 - acc: 0.9908 - val_loss: 0.0334 - val_acc: 0.9901\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0286 - acc: 0.9912 - val_loss: 0.0298 - val_acc: 0.9905\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 146us/step - loss: 0.0272 - acc: 0.9916 - val_loss: 0.0284 - val_acc: 0.9903\n",
            "Test loss: 0.028430081488785255\n",
            "Test accuracy: 0.9903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRjBpA2oPcMw",
        "colab_type": "text"
      },
      "source": [
        "2. Print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB__cb0MGmK7",
        "colab_type": "code",
        "outputId": "b57d79d9-33b5-470f-de60-d5aaff47c0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = model.predict_classes(x_test)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "matrix = confusion_matrix(y_test, y_predict)\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 977    0    0    0    0    1    1    1    0    0]\n",
            " [   0 1132    1    0    0    0    2    0    0    0]\n",
            " [   1    2 1022    0    0    0    1    6    0    0]\n",
            " [   0    0    1 1003    0    3    0    2    1    0]\n",
            " [   0    0    0    0  964    0    4    0    1   13]\n",
            " [   1    0    0    5    0  885    1    0    0    0]\n",
            " [   5    2    0    0    1    3  946    0    1    0]\n",
            " [   0    0    4    1    0    0    0 1021    1    1]\n",
            " [   2    0    2    1    0    1    1    3  961    3]\n",
            " [   3    0    0    1    2    4    0    4    3  992]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEfv2uUkPk3y",
        "colab_type": "text"
      },
      "source": [
        "3. Perform Batch normalization at the very begining of the CNN network. And the test accuracy is 0.9908. Which is higher then above result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-FGejayGzyf",
        "colab_type": "code",
        "outputId": "3d6dbeff-38c9-4dbc-c89c-627a4322895e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "#Batch normalization\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# add batch normolization\n",
        "model = Sequential()\n",
        "#batch normalization\n",
        "model.add(BatchNormalization()) # add here\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 10s 167us/step - loss: 0.2165 - acc: 0.9325 - val_loss: 0.0503 - val_acc: 0.9831\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0763 - acc: 0.9775 - val_loss: 0.0360 - val_acc: 0.9882\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0586 - acc: 0.9822 - val_loss: 0.0325 - val_acc: 0.9895\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0471 - acc: 0.9858 - val_loss: 0.0307 - val_acc: 0.9902\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0403 - acc: 0.9881 - val_loss: 0.0286 - val_acc: 0.9911\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0341 - acc: 0.9897 - val_loss: 0.0292 - val_acc: 0.9901\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0309 - acc: 0.9910 - val_loss: 0.0305 - val_acc: 0.9912\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.0277 - acc: 0.9919 - val_loss: 0.0324 - val_acc: 0.9907\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.0280 - acc: 0.9910 - val_loss: 0.0269 - val_acc: 0.9912\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0281 - val_acc: 0.9912\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 156us/step - loss: 0.0235 - acc: 0.9927 - val_loss: 0.0312 - val_acc: 0.9922\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 155us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0289 - val_acc: 0.9908\n",
            "Test loss: 0.02887618561537547\n",
            "Test accuracy: 0.9908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqzXl6g_P_Kc",
        "colab_type": "text"
      },
      "source": [
        "4. Perform image standardization to the CNN network, and the test accuracy is 0.9918, again higher then before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntj5ht14NEou",
        "colab_type": "code",
        "outputId": "77a72f09-5ba7-4331-8ae0-817ebdff068f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Image standardization\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#image standardization\n",
        "#1. feature-wise generator\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    )\n",
        "#2. calculate mean and standard deviation on the training dataset\n",
        "datagen.fit(x_train)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.2640 - acc: 0.9193 - val_loss: 0.0524 - val_acc: 0.9824\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0856 - acc: 0.9739 - val_loss: 0.0386 - val_acc: 0.9875\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.0626 - acc: 0.9817 - val_loss: 0.0350 - val_acc: 0.9882\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0522 - acc: 0.9842 - val_loss: 0.0276 - val_acc: 0.9897\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.0463 - acc: 0.9862 - val_loss: 0.0291 - val_acc: 0.9896\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 9s 151us/step - loss: 0.0400 - acc: 0.9877 - val_loss: 0.0283 - val_acc: 0.9906\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0358 - acc: 0.9890 - val_loss: 0.0261 - val_acc: 0.9915\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0327 - acc: 0.9901 - val_loss: 0.0260 - val_acc: 0.9917\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.0284 - val_acc: 0.9909\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 9s 149us/step - loss: 0.0282 - acc: 0.9914 - val_loss: 0.0272 - val_acc: 0.9916\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 9s 147us/step - loss: 0.0257 - acc: 0.9916 - val_loss: 0.0262 - val_acc: 0.9916\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 9s 152us/step - loss: 0.0252 - acc: 0.9927 - val_loss: 0.0264 - val_acc: 0.9918\n",
            "Test loss: 0.02644279885970409\n",
            "Test accuracy: 0.9918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEC2E2dqQMhF",
        "colab_type": "text"
      },
      "source": [
        "5. Perform both Batch Normalization and Image Standardization to the CNN network, and the accuracy is 0.9917, almost the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGNs8r-DSlcJ",
        "colab_type": "code",
        "outputId": "14958602-6436-49f1-e03b-aa683ef30309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Both Batch Normalization and Image standardization\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#image standardization\n",
        "#1. feature-wise generator\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    #rotation_range=20,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True\n",
        "    )\n",
        "#2. calculate mean and standard deviation on the training dataset\n",
        "datagen.fit(x_train)\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "#model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "#                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 10s 174us/step - loss: 0.2364 - acc: 0.9273 - val_loss: 0.0552 - val_acc: 0.9817\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0835 - acc: 0.9753 - val_loss: 0.0460 - val_acc: 0.9845\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0610 - acc: 0.9822 - val_loss: 0.0315 - val_acc: 0.9894\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0496 - acc: 0.9852 - val_loss: 0.0284 - val_acc: 0.9904\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 9s 157us/step - loss: 0.0423 - acc: 0.9867 - val_loss: 0.0341 - val_acc: 0.9883\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 10s 160us/step - loss: 0.0370 - acc: 0.9885 - val_loss: 0.0301 - val_acc: 0.9900\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0335 - acc: 0.9900 - val_loss: 0.0336 - val_acc: 0.9904\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0321 - acc: 0.9903 - val_loss: 0.0285 - val_acc: 0.9915\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 9s 158us/step - loss: 0.0294 - acc: 0.9908 - val_loss: 0.0274 - val_acc: 0.9914\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 10s 159us/step - loss: 0.0271 - acc: 0.9916 - val_loss: 0.0311 - val_acc: 0.9910\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 10s 161us/step - loss: 0.0262 - acc: 0.9924 - val_loss: 0.0323 - val_acc: 0.9906\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 10s 162us/step - loss: 0.0248 - acc: 0.9923 - val_loss: 0.0280 - val_acc: 0.9917\n",
            "Test loss: 0.027967271339866057\n",
            "Test accuracy: 0.9917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voEXbqswQZhL",
        "colab_type": "text"
      },
      "source": [
        "6. Then we add a new convolutional layer to the CNN, and perform no Batch Normalization and Image Standardization. The test accuracy is 0.9944, which is among the highest of the above designs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J36aG7d9-Ihe",
        "colab_type": "code",
        "outputId": "a4b877cc-0de4-46a3-fcbd-ac470636f576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 16s 260us/step - loss: 0.2596 - acc: 0.9201 - val_loss: 0.0517 - val_acc: 0.9847\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0775 - acc: 0.9763 - val_loss: 0.0375 - val_acc: 0.9867\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0573 - acc: 0.9831 - val_loss: 0.0316 - val_acc: 0.9891\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0494 - acc: 0.9860 - val_loss: 0.0239 - val_acc: 0.9921\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0413 - acc: 0.9877 - val_loss: 0.0247 - val_acc: 0.9921\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 15s 250us/step - loss: 0.0351 - acc: 0.9899 - val_loss: 0.0241 - val_acc: 0.9917\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0314 - acc: 0.9906 - val_loss: 0.0214 - val_acc: 0.9928\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0276 - acc: 0.9919 - val_loss: 0.0210 - val_acc: 0.9937\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 15s 246us/step - loss: 0.0263 - acc: 0.9918 - val_loss: 0.0192 - val_acc: 0.9936\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0237 - acc: 0.9926 - val_loss: 0.0247 - val_acc: 0.9922\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0231 - acc: 0.9928 - val_loss: 0.0245 - val_acc: 0.9920\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0192 - val_acc: 0.9944\n",
            "Test loss: 0.019247188913997616\n",
            "Test accuracy: 0.9944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiRjF8BtQw4X",
        "colab_type": "text"
      },
      "source": [
        "7. Then we add a new convolutional layer to the CNN, and perform both Batch Normalization and Image Standardization. The test accuracy is 0.994, which is among the highest of the above designs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0mFD455iA3",
        "colab_type": "code",
        "outputId": "21436041-e388-4ca7-c79e-105b5e5d331f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Both Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#image standardization\n",
        "#1. feature-wise generator\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    )\n",
        "#2. calculate mean and standard deviation on the training dataset\n",
        "datagen.fit(x_train)\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), #convolution layer 1\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) #activate layer 1\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), #convolution layer 2\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) #activate layer 2\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 20s 331us/step - loss: 0.2165 - acc: 0.9344 - val_loss: 0.0456 - val_acc: 0.9852\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 16s 259us/step - loss: 0.0709 - acc: 0.9794 - val_loss: 0.0484 - val_acc: 0.9838\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0529 - acc: 0.9841 - val_loss: 0.0276 - val_acc: 0.9905\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0430 - acc: 0.9871 - val_loss: 0.0290 - val_acc: 0.9901\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0366 - acc: 0.9893 - val_loss: 0.0245 - val_acc: 0.9928\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0315 - acc: 0.9903 - val_loss: 0.0216 - val_acc: 0.9934\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0274 - acc: 0.9919 - val_loss: 0.0215 - val_acc: 0.9934\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0255 - acc: 0.9919 - val_loss: 0.0249 - val_acc: 0.9923\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.0227 - val_acc: 0.9936\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 15s 258us/step - loss: 0.0221 - acc: 0.9934 - val_loss: 0.0260 - val_acc: 0.9932\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 15s 255us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0210 - val_acc: 0.9935\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0182 - acc: 0.9944 - val_loss: 0.0205 - val_acc: 0.9940\n",
            "Test loss: 0.020458687047763213\n",
            "Test accuracy: 0.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjI1hiDpQ8i4",
        "colab_type": "text"
      },
      "source": [
        "8. Then we add two new convolutional layer to the CNN, and perform both Batch Normalization and Image Standardization. The test accuracy is 0.9931, which is among the highest of the above designs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xZlqfOq8Jhe",
        "colab_type": "code",
        "outputId": "451faa5c-8a83-4659-f80c-ed22de0cc280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Both Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#Add a new convoulutional layer\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "#image standardization\n",
        "#1. feature-wise generator\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    #rotation_range=20,\n",
        "    #width_shift_range=0.2,\n",
        "    #height_shift_range=0.2,\n",
        "    #horizontal_flip=True\n",
        "    )\n",
        "#2. calculate mean and standard deviation on the training dataset\n",
        "datagen.fit(x_train)\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "#model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
        "#                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
        "model = Sequential()\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), #convolution layer 1\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) #activate layer 1\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), #convolution layer 2\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu')) #activate layer 2\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.2415 - acc: 0.9253 - val_loss: 0.0642 - val_acc: 0.9798\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0782 - acc: 0.9762 - val_loss: 0.0318 - val_acc: 0.9886\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0556 - acc: 0.9836 - val_loss: 0.0300 - val_acc: 0.9897\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 15s 254us/step - loss: 0.0452 - acc: 0.9871 - val_loss: 0.0252 - val_acc: 0.9897\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0382 - acc: 0.9889 - val_loss: 0.0294 - val_acc: 0.9900\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0334 - acc: 0.9905 - val_loss: 0.0222 - val_acc: 0.9925\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0298 - acc: 0.9910 - val_loss: 0.0236 - val_acc: 0.9914\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0278 - acc: 0.9918 - val_loss: 0.0228 - val_acc: 0.9930\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0248 - acc: 0.9927 - val_loss: 0.0242 - val_acc: 0.9929\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0220 - acc: 0.9932 - val_loss: 0.0201 - val_acc: 0.9943\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 15s 256us/step - loss: 0.0218 - acc: 0.9933 - val_loss: 0.0231 - val_acc: 0.9922\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 15s 257us/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0232 - val_acc: 0.9931\n",
            "Test loss: 0.023153055907657108\n",
            "Test accuracy: 0.9931\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rme3RH1RFVh",
        "colab_type": "text"
      },
      "source": [
        "9. As we found that no Batch Normalization and Image Standardization, but just add one more layer to the CNN can perform the best, so we choose this design as the base, then change the value of the batch size and epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5mw43WfAJKh",
        "colab_type": "code",
        "outputId": "40feac3f-b6b2-413b-f671-0faff71946e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change batch size to 64, epoch is 12\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 20s 338us/step - loss: 0.1917 - acc: 0.9413 - val_loss: 0.0369 - val_acc: 0.9879\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0683 - acc: 0.9807 - val_loss: 0.0349 - val_acc: 0.9889\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0512 - acc: 0.9842 - val_loss: 0.0256 - val_acc: 0.9907\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0434 - acc: 0.9871 - val_loss: 0.0276 - val_acc: 0.9903\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.0383 - acc: 0.9890 - val_loss: 0.0244 - val_acc: 0.9917\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 19s 319us/step - loss: 0.0329 - acc: 0.9910 - val_loss: 0.0252 - val_acc: 0.9915\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0302 - acc: 0.9915 - val_loss: 0.0237 - val_acc: 0.9920\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.0286 - acc: 0.9919 - val_loss: 0.0208 - val_acc: 0.9928\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0253 - acc: 0.9925 - val_loss: 0.0213 - val_acc: 0.9920\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.0222 - acc: 0.9934 - val_loss: 0.0284 - val_acc: 0.9936\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 19s 318us/step - loss: 0.0226 - acc: 0.9935 - val_loss: 0.0234 - val_acc: 0.9929\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 19s 317us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0222 - val_acc: 0.9940\n",
            "Test loss: 0.022207465385138767\n",
            "Test accuracy: 0.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maDFHU6aBOE9",
        "colab_type": "code",
        "outputId": "01dd7715-72f5-465e-bcd2-4f462c479a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change batch size to 256, epoch is 12\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 13s 216us/step - loss: 0.3539 - acc: 0.8915 - val_loss: 0.0580 - val_acc: 0.9811\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0924 - acc: 0.9722 - val_loss: 0.0380 - val_acc: 0.9868\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.0651 - acc: 0.9810 - val_loss: 0.0404 - val_acc: 0.9859\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0546 - acc: 0.9835 - val_loss: 0.0316 - val_acc: 0.9891\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0465 - acc: 0.9863 - val_loss: 0.0277 - val_acc: 0.9906\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.0395 - acc: 0.9880 - val_loss: 0.0347 - val_acc: 0.9882\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0349 - acc: 0.9892 - val_loss: 0.0245 - val_acc: 0.9915\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0328 - acc: 0.9906 - val_loss: 0.0258 - val_acc: 0.9920\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.0226 - val_acc: 0.9920\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0251 - acc: 0.9923 - val_loss: 0.0242 - val_acc: 0.9922\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0226 - acc: 0.9931 - val_loss: 0.0260 - val_acc: 0.9914\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 12s 194us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0227 - val_acc: 0.9939\n",
            "Test loss: 0.022688690705556836\n",
            "Test accuracy: 0.9939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1SwmDRwBl2n",
        "colab_type": "code",
        "outputId": "7f61b5b3-e092-4032-8fc2-97717c089fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 4, batch size is 128\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 16s 269us/step - loss: 0.2751 - acc: 0.9151 - val_loss: 0.0463 - val_acc: 0.9844\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0788 - acc: 0.9775 - val_loss: 0.0298 - val_acc: 0.9895\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0575 - acc: 0.9831 - val_loss: 0.0284 - val_acc: 0.9903\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0456 - acc: 0.9868 - val_loss: 0.0236 - val_acc: 0.9916\n",
            "Test loss: 0.02355459110240481\n",
            "Test accuracy: 0.9916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKxQ1dfCTl2x",
        "colab_type": "code",
        "outputId": "7b93bcbe-77d1-4077-d52d-06c58142ef6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 8, batch size is 128\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 8\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/8\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.2431 - acc: 0.9246 - val_loss: 0.0453 - val_acc: 0.9849\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0757 - acc: 0.9776 - val_loss: 0.0319 - val_acc: 0.9888\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 15s 250us/step - loss: 0.0575 - acc: 0.9832 - val_loss: 0.0295 - val_acc: 0.9906\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0487 - acc: 0.9859 - val_loss: 0.0257 - val_acc: 0.9907\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0400 - acc: 0.9886 - val_loss: 0.0235 - val_acc: 0.9926\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 15s 251us/step - loss: 0.0358 - acc: 0.9895 - val_loss: 0.0235 - val_acc: 0.9922\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0330 - acc: 0.9901 - val_loss: 0.0215 - val_acc: 0.9924\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0282 - acc: 0.9914 - val_loss: 0.0188 - val_acc: 0.9935\n",
            "Test loss: 0.01875828201958211\n",
            "Test accuracy: 0.9935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR1T38IZUI2a",
        "colab_type": "code",
        "outputId": "7de67ef4-7605-4384-f117-87d8bc4e430f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 16, batch size is 128\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 16\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/16\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.2607 - acc: 0.9195 - val_loss: 0.0491 - val_acc: 0.9833\n",
            "Epoch 2/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0812 - acc: 0.9759 - val_loss: 0.0391 - val_acc: 0.9866\n",
            "Epoch 3/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0564 - acc: 0.9835 - val_loss: 0.0300 - val_acc: 0.9894\n",
            "Epoch 4/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0479 - acc: 0.9854 - val_loss: 0.0301 - val_acc: 0.9911\n",
            "Epoch 5/16\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0415 - acc: 0.9882 - val_loss: 0.0239 - val_acc: 0.9931\n",
            "Epoch 6/16\n",
            "60000/60000 [==============================] - 15s 249us/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.0211 - val_acc: 0.9932\n",
            "Epoch 7/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0315 - acc: 0.9909 - val_loss: 0.0233 - val_acc: 0.9928\n",
            "Epoch 8/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0286 - acc: 0.9916 - val_loss: 0.0229 - val_acc: 0.9933\n",
            "Epoch 9/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0265 - acc: 0.9920 - val_loss: 0.0233 - val_acc: 0.9930\n",
            "Epoch 10/16\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0249 - acc: 0.9923 - val_loss: 0.0227 - val_acc: 0.9933\n",
            "Epoch 11/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0218 - val_acc: 0.9936\n",
            "Epoch 12/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0257 - val_acc: 0.9920\n",
            "Epoch 13/16\n",
            "60000/60000 [==============================] - 15s 248us/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0255 - val_acc: 0.9936\n",
            "Epoch 14/16\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.0231 - val_acc: 0.9936\n",
            "Epoch 15/16\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0227 - val_acc: 0.9931\n",
            "Epoch 16/16\n",
            "60000/60000 [==============================] - 15s 247us/step - loss: 0.0167 - acc: 0.9947 - val_loss: 0.0213 - val_acc: 0.9928\n",
            "Test loss: 0.021335434461147905\n",
            "Test accuracy: 0.9928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxLVZEIdURVx",
        "colab_type": "code",
        "outputId": "7b3737d7-53d3-4600-f2a5-46dffc1e466e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 4, batch size is 64\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 21s 350us/step - loss: 0.1964 - acc: 0.9405 - val_loss: 0.0481 - val_acc: 0.9846\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.0701 - acc: 0.9794 - val_loss: 0.0344 - val_acc: 0.9890\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0514 - acc: 0.9845 - val_loss: 0.0247 - val_acc: 0.9908\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0432 - acc: 0.9872 - val_loss: 0.0249 - val_acc: 0.9920\n",
            "Test loss: 0.02493820607790476\n",
            "Test accuracy: 0.992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFfEq6WBUl58",
        "colab_type": "code",
        "outputId": "b89614a1-19da-44eb-917f-0ad23fc2adc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 8, batch size is 64\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 8\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/8\n",
            "60000/60000 [==============================] - 21s 351us/step - loss: 0.2011 - acc: 0.9382 - val_loss: 0.0483 - val_acc: 0.9854\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.0696 - acc: 0.9795 - val_loss: 0.0366 - val_acc: 0.9875\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0514 - acc: 0.9851 - val_loss: 0.0263 - val_acc: 0.9912\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0432 - acc: 0.9874 - val_loss: 0.0257 - val_acc: 0.9906\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.0372 - acc: 0.9891 - val_loss: 0.0224 - val_acc: 0.9925\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0329 - acc: 0.9901 - val_loss: 0.0213 - val_acc: 0.9935\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0302 - acc: 0.9911 - val_loss: 0.0206 - val_acc: 0.9931\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0266 - acc: 0.9922 - val_loss: 0.0326 - val_acc: 0.9891\n",
            "Test loss: 0.03263401714714891\n",
            "Test accuracy: 0.9891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNXURCbMUuPu",
        "colab_type": "code",
        "outputId": "a82b2caa-4d54-4e24-dcc8-cfbbf3850ad0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 16, batch size is 64\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 16\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/16\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.2154 - acc: 0.9355 - val_loss: 0.0525 - val_acc: 0.9820\n",
            "Epoch 2/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0713 - acc: 0.9795 - val_loss: 0.0354 - val_acc: 0.9883\n",
            "Epoch 3/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0545 - acc: 0.9844 - val_loss: 0.0294 - val_acc: 0.9908\n",
            "Epoch 4/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0458 - acc: 0.9865 - val_loss: 0.0280 - val_acc: 0.9898\n",
            "Epoch 5/16\n",
            "60000/60000 [==============================] - 20s 327us/step - loss: 0.0396 - acc: 0.9887 - val_loss: 0.0255 - val_acc: 0.9919\n",
            "Epoch 6/16\n",
            "60000/60000 [==============================] - 20s 325us/step - loss: 0.0358 - acc: 0.9893 - val_loss: 0.0289 - val_acc: 0.9905\n",
            "Epoch 7/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0317 - acc: 0.9906 - val_loss: 0.0231 - val_acc: 0.9924\n",
            "Epoch 8/16\n",
            "60000/60000 [==============================] - 19s 323us/step - loss: 0.0290 - acc: 0.9912 - val_loss: 0.0221 - val_acc: 0.9929\n",
            "Epoch 9/16\n",
            "60000/60000 [==============================] - 19s 324us/step - loss: 0.0267 - acc: 0.9920 - val_loss: 0.0239 - val_acc: 0.9925\n",
            "Epoch 10/16\n",
            "60000/60000 [==============================] - 19s 320us/step - loss: 0.0236 - acc: 0.9930 - val_loss: 0.0222 - val_acc: 0.9937\n",
            "Epoch 11/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0235 - acc: 0.9933 - val_loss: 0.0220 - val_acc: 0.9935\n",
            "Epoch 12/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.0219 - val_acc: 0.9935\n",
            "Epoch 13/16\n",
            "60000/60000 [==============================] - 19s 325us/step - loss: 0.0217 - acc: 0.9930 - val_loss: 0.0221 - val_acc: 0.9934\n",
            "Epoch 14/16\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0199 - acc: 0.9942 - val_loss: 0.0206 - val_acc: 0.9945\n",
            "Epoch 15/16\n",
            "60000/60000 [==============================] - 19s 322us/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.0244 - val_acc: 0.9927\n",
            "Epoch 16/16\n",
            "60000/60000 [==============================] - 19s 321us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0239 - val_acc: 0.9936\n",
            "Test loss: 0.02385415731826074\n",
            "Test accuracy: 0.9936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLZPDdniU-LW",
        "colab_type": "code",
        "outputId": "2f2d8612-6dbd-49f3-a7f4-91923f2a8ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 4, batch size is 256\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 4\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 14s 225us/step - loss: 0.3458 - acc: 0.8919 - val_loss: 0.1046 - val_acc: 0.9690\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0954 - acc: 0.9715 - val_loss: 0.0406 - val_acc: 0.9867\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0690 - acc: 0.9792 - val_loss: 0.0357 - val_acc: 0.9871\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0542 - acc: 0.9837 - val_loss: 0.0344 - val_acc: 0.9882\n",
            "Test loss: 0.03436271249791607\n",
            "Test accuracy: 0.9882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvwzUukLVL6t",
        "colab_type": "code",
        "outputId": "750e06b2-f066-4601-e60e-c0b4dab6e333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 8, batch size is 256\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 8\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/8\n",
            "60000/60000 [==============================] - 14s 228us/step - loss: 0.3708 - acc: 0.8837 - val_loss: 0.0629 - val_acc: 0.9795\n",
            "Epoch 2/8\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0945 - acc: 0.9719 - val_loss: 0.0387 - val_acc: 0.9872\n",
            "Epoch 3/8\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0695 - acc: 0.9798 - val_loss: 0.0328 - val_acc: 0.9886\n",
            "Epoch 4/8\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0560 - acc: 0.9836 - val_loss: 0.0266 - val_acc: 0.9915\n",
            "Epoch 5/8\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0464 - acc: 0.9861 - val_loss: 0.0294 - val_acc: 0.9897\n",
            "Epoch 6/8\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0413 - acc: 0.9875 - val_loss: 0.0241 - val_acc: 0.9920\n",
            "Epoch 7/8\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0369 - acc: 0.9886 - val_loss: 0.0300 - val_acc: 0.9905\n",
            "Epoch 8/8\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0318 - acc: 0.9906 - val_loss: 0.0233 - val_acc: 0.9926\n",
            "Test loss: 0.023299644500946307\n",
            "Test accuracy: 0.9926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrgo29vUVQp8",
        "colab_type": "code",
        "outputId": "4189a186-6629-4779-87a8-2d2f9915ffd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "#change epoch to 16, batch size is 256\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 256\n",
        "num_classes = 10\n",
        "epochs = 16\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/16\n",
            "60000/60000 [==============================] - 14s 229us/step - loss: 0.3638 - acc: 0.8866 - val_loss: 0.0703 - val_acc: 0.9777\n",
            "Epoch 2/16\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0918 - acc: 0.9727 - val_loss: 0.0345 - val_acc: 0.9888\n",
            "Epoch 3/16\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0656 - acc: 0.9807 - val_loss: 0.0304 - val_acc: 0.9892\n",
            "Epoch 4/16\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0539 - acc: 0.9848 - val_loss: 0.0306 - val_acc: 0.9900\n",
            "Epoch 5/16\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0446 - acc: 0.9862 - val_loss: 0.0295 - val_acc: 0.9905\n",
            "Epoch 6/16\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.0389 - acc: 0.9884 - val_loss: 0.0245 - val_acc: 0.9925\n",
            "Epoch 7/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0341 - acc: 0.9896 - val_loss: 0.0226 - val_acc: 0.9931\n",
            "Epoch 8/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0309 - acc: 0.9904 - val_loss: 0.0254 - val_acc: 0.9924\n",
            "Epoch 9/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0277 - acc: 0.9917 - val_loss: 0.0200 - val_acc: 0.9932\n",
            "Epoch 10/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0261 - acc: 0.9918 - val_loss: 0.0218 - val_acc: 0.9936\n",
            "Epoch 11/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0245 - acc: 0.9921 - val_loss: 0.0251 - val_acc: 0.9919\n",
            "Epoch 12/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0222 - acc: 0.9928 - val_loss: 0.0256 - val_acc: 0.9915\n",
            "Epoch 13/16\n",
            "60000/60000 [==============================] - 12s 196us/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0231 - val_acc: 0.9926\n",
            "Epoch 14/16\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.0228 - val_acc: 0.9939\n",
            "Epoch 15/16\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0242 - val_acc: 0.9927\n",
            "Epoch 16/16\n",
            "60000/60000 [==============================] - 12s 195us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0238 - val_acc: 0.9930\n",
            "Test loss: 0.02378382302098471\n",
            "Test accuracy: 0.993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAh0_w0Cfyn",
        "colab_type": "code",
        "outputId": "e17659d6-a890-482e-f61e-c2b4a378dc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "fig = plt.figure()\n",
        "col_labels = ['epoch is 4','epoch is 8','epoch is 12','epoch is 16']\n",
        "row_labels = ['batch is 64','batch is 128','batch is 256']\n",
        "table_vals = [[0.992,0.9891,0.994,0.9936],[0.9916,0.9935,0.9944,0.9928],[0.9882,0.9926,0.9939,0.993]]\n",
        "\n",
        "the_table = plt.table(cellText=table_vals,\n",
        "                      colWidths=[0.1] * 3,\n",
        "                      rowLabels=row_labels,\n",
        "                      colLabels=col_labels,\n",
        "                      loc='center')\n",
        "the_table.auto_set_font_size(False)\n",
        "the_table.set_fontsize(24)\n",
        "the_table.scale(4, 5)\n",
        "# Removing ticks and spines enables you to get the figure only with table\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
        "for pos in ['right','top','bottom','left']:\n",
        "    plt.gca().spines[pos].set_visible(False)\n",
        "plt.savefig('matplotlib-table.png', bbox_inches='tight', pad_inches=0.05)'''\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "col_labels = ['epoch is 4','epoch is 8','epoch is 12','epoch is 16']\n",
        "row_labels = ['batch is 64','batch is 128','batch is 256']\n",
        "table_vals = [[0.992,0.9891,0.994,0.9936],[0.9916,0.9935,0.9944,0.9928],[0.9882,0.9926,0.9939,0.993]]\n",
        "\n",
        "# Draw table\n",
        "the_table = plt.table(cellText=table_vals,\n",
        "                      colWidths=[0.1] * 4,\n",
        "                      rowLabels=row_labels,\n",
        "                      colLabels=col_labels,\n",
        "                      loc='center'\n",
        "                      )\n",
        "the_table.auto_set_font_size(False)\n",
        "the_table.set_fontsize(24)\n",
        "the_table.scale(7, 7)\n",
        "\n",
        "# Removing ticks and spines enables you to get the figure only with table\n",
        "plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
        "plt.tick_params(axis='y', which='both', right=False, left=False, labelleft=False)\n",
        "for pos in ['right','top','bottom','left']:\n",
        "    plt.gca().spines[pos].set_visible(False)\n",
        "plt.savefig('matplotlib-table.png', bbox_inches='tight', pad_inches=0.05)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABGkAAAFeCAYAAAA2U0IZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLvB/ElYiLbJNJB0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl5rDgeyRvXi",
        "colab_type": "text"
      },
      "source": [
        "10. From the table above, we can see that when the batch is 128, and theepoch is 12, the CNN can perform the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtskM-qHMMPG",
        "colab_type": "code",
        "outputId": "47af9976-b669-4b2a-ab70-79dc8e26e351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "\n",
        "#No Batch Normalization and Image standardization\n",
        "#Add a new convoulutional layer\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.2561 - acc: 0.9214 - val_loss: 0.0534 - val_acc: 0.9823\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0808 - acc: 0.9760 - val_loss: 0.0367 - val_acc: 0.9869\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0591 - acc: 0.9828 - val_loss: 0.0321 - val_acc: 0.9892\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0499 - acc: 0.9850 - val_loss: 0.0283 - val_acc: 0.9907\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0422 - acc: 0.9876 - val_loss: 0.0261 - val_acc: 0.9913\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0371 - acc: 0.9893 - val_loss: 0.0281 - val_acc: 0.9906\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0326 - acc: 0.9902 - val_loss: 0.0226 - val_acc: 0.9920\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0298 - acc: 0.9909 - val_loss: 0.0251 - val_acc: 0.9910\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0267 - acc: 0.9918 - val_loss: 0.0232 - val_acc: 0.9930\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0248 - acc: 0.9927 - val_loss: 0.0226 - val_acc: 0.9923\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0229 - acc: 0.9931 - val_loss: 0.0233 - val_acc: 0.9926\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0213 - acc: 0.9934 - val_loss: 0.0232 - val_acc: 0.9926\n",
            "Test loss: 0.02316287504988022\n",
            "Test accuracy: 0.9926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jlcBY80NrAx",
        "colab_type": "code",
        "outputId": "89178eb0-cec4-43f2-b738-fc0d8e085bbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_predict = model.predict_classes(x_test)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "matrix = confusion_matrix(y_test, y_predict)\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 977    0    1    0    0    0    1    1    0    0]\n",
            " [   0 1133    0    0    0    1    0    1    0    0]\n",
            " [   1    2 1023    0    0    0    0    6    0    0]\n",
            " [   0    0    1 1008    0    1    0    0    0    0]\n",
            " [   0    0    0    0  979    0    1    0    0    2]\n",
            " [   1    0    0    8    0  881    1    1    0    0]\n",
            " [   5    3    0    0    2    3  945    0    0    0]\n",
            " [   0    2    3    1    0    0    0 1020    1    1]\n",
            " [   2    0    2    1    0    1    0    1  965    2]\n",
            " [   0    0    0    0    5    4    0    4    1  995]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}